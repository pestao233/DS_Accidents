import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder
from scipy.stats import chi2_contingency
import plotly.express as px
import warnings
warnings.filterwarnings("ignore")
import io
from PIL import Image

# 1) largeur de page : "centered" ou "wide"
st.set_page_config(layout="centered", page_title="Accidents routiers", page_icon="üöß")

# 2) CSS simple : largeur max + tailles + espacement
st.markdown("""
<style>
/* largeur max du contenu */
.main .block-container {max-width: 1100px; padding-top: 1.5rem; padding-bottom: 4rem;}
/* titres */
h1, h2, h3 { letter-spacing: .2px; }
h1 { font-size: 1.9rem !important; }
h2 { font-size: 1.4rem !important; margin-top: 1.2rem; }
/* paragraphes */
p, li { line-height: 1.6; font-size: 0.98rem; }
/* s√©parateurs plus discrets */
hr { margin: .8rem 0 1.2rem 0; opacity: .4; }
/* badges */
.badge { display:inline-block; padding: .15rem .5rem; border-radius: .5rem; background:#111827; color:#E5E7EB; font-size:.75rem; }
.card { border:1px solid rgba(255,255,255,.1); border-radius: 12px; padding: .9rem 1rem; margin:.4rem 0; }
</style>
""", unsafe_allow_html=True)

#########

# Petits badges
st.markdown('<span class="badge">Dataset 2005‚Äì2018</span> '
            '<span class="badge">Classification</span> '
            '<span class="badge">Streamlit Demo</span>', unsafe_allow_html=True)

st.divider()



########################################
st.title("Exploration des donn√©es")

st.markdown("<u><font size=5>__Cadre__</u>", unsafe_allow_html=True)
st.write("""
Les donn√©es proviennent de la base publique des accidents corporels de la circulation en France, disponibles via data.gouv.fr. 
Nous avons utilis√© les fichiers annuels entre 2005 et 2018, structur√©s en quatre tables :
- **caracteristiques** : infos principales sur l‚Äôaccident (date, heure, luminosit√©, lieu, m√©t√©o, etc.)
- **lieux** : configuration de la route et conditions d'infrastructure
- **vehicules** : type de v√©hicules impliqu√©s et point d'impact
- **usagers** : profil et gravit√© des personnes impliqu√©es\n
Une difficult√© notable a √©t√© la gestion de l‚Äôh√©t√©rog√©n√©it√© entre fichiers annuels : encodages variables (latin1 ou utf-8), s√©parateurs diff√©rents (, ou \t), colonnes pr√©sentes ou absentes selon l'ann√©e.

Un script de lecture automatique a √©t√© mis en place pour charger les 4 tables par ann√©e en homog√©n√©isant les formats.\n
La volum√©trie est importante : chaque fichier annuel contient plusieurs dizaines de milliers de lignes, pour un total d‚Äôenviron 1 million d‚Äôaccidents sur la p√©riode √©tudi√©e.\n
Nous avons aussi contr√¥l√© la coh√©rence des identifiants Num_Acc entre les tables et aucune diff√©rence identifi√©e.\n
Nous avons √©galement dans ces fichiers beaucoup de variables qui ne sont pas pertinentes pour notre √©tude.
Il a donc fallu √©valuer leur pertinence afin de d√©cider de les garder ou non.
""")

#############################################################################
##                      Caract√©ristiques                                   ##
#############################################################################

#st.markdown("<u><font size=5>__Caract√©ristiques__</font></u>", unsafe_allow_html=True)

@st.cache_data
def load_caracteristiques_2005_2018():
    caracteristiques_2005_2018 = []
    dtypes_caracs = {
        "dep": "category", "com": "category",
        "lum": "int8", "agg": "int8", "int": "int8",
        "atm": "int8", "col": "int8",
        "mois": "int8", "jour": "int8", "an": "int16"
    }

    for annee in range(2005, 2019):                                            #### Filtrer jusqu'en 2009 au lieu de 2019 (pour les tests)
        chemin = f'data/sample_caracteristiques_{annee}.csv'  
        #sep = '\t' if annee == 2009 else ','
        sep=','
        df = pd.read_csv(
            chemin, sep=',', encoding='latin1',
            dtype={'dep': str, 'com': str, 'hrmn': str}  
        )
        df['annee'] = annee
        caracteristiques_2005_2018.append(df)

    caracs = pd.concat(caracteristiques_2005_2018, ignore_index=True)
    return caracs

# Appel
caracs = load_caracteristiques_2005_2018()


#############################################################################
##                              Usagers                                    ##
#############################################################################
#st.markdown("<u><font size=5>__Usagers__</font></u>", unsafe_allow_html=True)

@st.cache_data
def load_usagers_2005_2018():
    usagers_2005_2018 = []
    for annee in range(2005, 2019):                                            #### Filtrer jusqu'en 2009 au lieu de 2019 (pour les tests)
        chemin = f'data/sample_usagers_{annee}.csv'
        df = pd.read_csv(chemin, sep=',', encoding='latin1')
        df['annee'] = annee
        usagers_2005_2018.append(df)

    usagers = pd.concat(usagers_2005_2018, ignore_index=True)
    return usagers

# Appel
usagers = load_usagers_2005_2018()



#############################################################################
##                              Lieux                                      ##
#############################################################################
#st.markdown("<u><font size=5>__Lieux__</font></u>", unsafe_allow_html=True)

@st.cache_data
def load_lieux_2005_2018():
    lieux_2005_2018 = []
    for annee in range(2005, 2019):                                            #### Filtrer jusqu'en 2009 au lieu de 2019 (pour les tests)
        chemin = f'data/sample_lieux_{annee}.csv'
        df = pd.read_csv(chemin, sep=',', encoding='latin1')
        df['annee'] = annee
        lieux_2005_2018.append(df)

    lieux = pd.concat(lieux_2005_2018, ignore_index=True)
    return lieux

# Appel
lieux = load_lieux_2005_2018()


#############################################################################
##                              Vehicules                                  ##
#############################################################################
#st.markdown("<u><font size=5>__Vehicules__</font></u>", unsafe_allow_html=True)

@st.cache_data
def load_vehicules_2005_2018():
    vehicules_2005_2018 = []
    for annee in range(2005, 2019):                                            #### Filtrer jusqu'en 2009 au lieu de 2019 (pour les tests)
        chemin = f'data/sample_vehicules_{annee}.csv'
        df = pd.read_csv(chemin, sep=',', encoding='latin1')
        df['annee'] = annee
        vehicules_2005_2018.append(df)

    vehicules = pd.concat(vehicules_2005_2018, ignore_index=True)
    return vehicules

# Appel
vehicules = load_vehicules_2005_2018()


#############################################################################
##                                Head/info                                ##
#############################################################################

st.divider()

# Onglets pour organiser ton code existant (colle tes blocs EDA dans les bons onglets)
tab1, tab2, tab3 = st.tabs(["üì• Chargement", "üîç Exploration / üßº Nettoyage", "üìä Dataviz"])
with tab1:
    st.markdown("#### Aper√ßu du DataFrame : `Caract√©ristiques`")
    st.dataframe(caracs.head())
    c1, c2, c3 = st.columns(3)
    c1.metric("Lignes totales (full):", "958 469")
    c2.metric("Lignes totales (sample):", f"{caracs.shape[0]:,}".replace(",", " "))
    c3.metric("Colonnes totales:", caracs.shape[1])
    #c3.metric("M√©moire (Mo)", round(caracs.memory_usage(deep=True).sum()/1024**2, 2))

    st.subheader("R√©sum√© du DataFrame : `Caract√©ristiques`")
    st.code("""
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 958469 entries, 0 to 958468
    Data columns (total 17 columns):
    #   Column   Non-Null Count   Dtype  
    ---  ------   --------------   -----  
    0   Num_Acc  958469 non-null  int64  
    1   an       958469 non-null  int64  
    2   mois     958469 non-null  int64  
    3   jour     958469 non-null  int64  
    4   hrmn     958469 non-null  object 
    5   lum      958469 non-null  int64  
    6   agg      958469 non-null  int64  
    7   int      958469 non-null  int64  
    8   atm      958396 non-null  float64
    9   col      958450 non-null  float64
    10  com      958467 non-null  object 
    11  adr      816550 non-null  object 
    12  gps      480052 non-null  object 
    13  lat      471401 non-null  float64
    14  long     471397 non-null  object 
    15  dep      958469 non-null  object 
    16  annee    958469 non-null  int64  
    dtypes: float64(3), int64(8), object(6)
    """)

    st.divider()

    st.markdown("#### Aper√ßu du DataFrame : `Usagers`")
    st.dataframe(usagers.head())
    c1, c2, c3 = st.columns(3)
    c1.metric("Lignes totales (full):", "2 142 195")
    c2.metric("Lignes totales (sample):", f"{usagers.shape[0]:,}".replace(",", " "))
    c3.metric("Colonnes totales:", usagers.shape[1])
    #c3.metric("M√©moire (Mo)", round(usagers.memory_usage(deep=True).sum()/1024**2, 2))
   
    st.subheader("R√©sum√© du DataFrame : `Usagers`")
    st.code("""
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 2142195 entries, 0 to 2142194
    Data columns (total 13 columns):
    #   Column   Dtype  
    ---  ------   -----  
    0   Num_Acc  int64  
    1   place    float64
    2   catu     int64  
    3   grav     int64  
    4   sexe     int64  
    5   trajet   float64
    6   secu     float64
    7   locp     float64
    8   actp     float64
    9   etatp    float64
    10  an_nais  float64
    11  num_veh  object 
    12  annee    int64  
    dtypes: float64(7), int64(5), object(1)
    """)
    
    st.divider()

    st.markdown("#### Aper√ßu du DataFrame : `Lieux`")
    st.dataframe(lieux.head())
    c1, c2, c3 = st.columns(3)
    c1.metric("Lignes totales (full):", "958 469")
    c2.metric("Lignes totales (sample):", f"{lieux.shape[0]:,}".replace(",", " "))
    c3.metric("Colonnes totales:", lieux.shape[1])
    #c3.metric("M√©moire (Mo)", round(lieux.memory_usage(deep=True).sum()/1024**2, 2))
            
    st.subheader("R√©sum√© du DataFrame : `Lieux`")
    st.code("""
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 958469 entries, 0 to 958468
    Data columns (total 19 columns):
    #   Column   Non-Null Count   Dtype  
    ---  ------   --------------   -----  
    0   Num_Acc  958469 non-null  int64  
    1   catr     958468 non-null  float64
    2   voie     869558 non-null  object 
    3   v1       333391 non-null  float64
    4   v2       39348 non-null   object 
    5   circ     956895 non-null  float64
    6   nbv      955738 non-null  float64
    7   pr       482985 non-null  float64
    8   pr1      481166 non-null  float64
    9   vosp     955708 non-null  float64
    10  prof     956520 non-null  float64
    11  plan     956188 non-null  float64
    12  lartpc   902271 non-null  float64
    13  larrout  904096 non-null  float64
    14  surf     956545 non-null  float64
    15  infra    953061 non-null  float64
    16  situ     953499 non-null  float64
    17  env1     953029 non-null  float64
    18  annee    958469 non-null  int64  
    dtypes: float64(15), int64(2), object(2)
    """)
    st.divider()

    st.markdown("#### Aper√ßu du DataFrame : `V√©hicules`")
    st.dataframe(vehicules.head())
    c1, c2, c3 = st.columns(3)
    c1.metric("Lignes totales (full):", "1 635 811")
    c2.metric("Lignes totales (sample):", f"{vehicules.shape[0]:,}".replace(",", " "))
    c3.metric("Colonnes totales:", vehicules.shape[1])
    #c3.metric("M√©moire (Mo)", round(vehicules.memory_usage(deep=True).sum()/1024**2, 2))  

    st.subheader("R√©sum√© du DataFrame `Vehicules`")
    st.code("""
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 1635811 entries, 0 to 1635810
    Data columns (total 10 columns):
    #   Column   Non-Null Count    Dtype  
    ---  ------   --------------    -----  
    0   Num_Acc  1635811 non-null  int64  
    1   senc     1635539 non-null  float64
    2   catv     1635811 non-null  int64  
    3   occutc   1635811 non-null  int64  
    4   obs      1634805 non-null  float64
    5   obsm     1635033 non-null  float64
    6   choc     1635414 non-null  float64
    7   manv     1635343 non-null  float64
    8   num_veh  1635811 non-null  object 
    9   annee    1635811 non-null  int64  
    dtypes: float64(5), int64(4), object(1)
    """)

            
with tab2:
    st.write("### Pr√©paration des donn√©es")
    st.write(""" Pour pr√©parer au mieux les donn√©es en vue de la mod√©lisation, nous avons test√© deux approches :
    - la premi√®re consistait √† pr√©traiter chaque DataFrame s√©par√©ment avant de les fusionner,
    - La seconde visait √† fusionner l‚Äôensemble des fichiers bruts avant d‚Äôappliquer un pr√©traitement global.
    Les deux m√©thodes ont conduit √† des r√©sultats relativement similaires. 
    
    Cependant, afin de limiter la perte d‚Äôinformations et de lignes d‚Äôaccidents lors du traitement post-fusion, nous avons retenu la seconde approche : fusionner puis traiter.
    """)

    st.code("""
    # On merge les diff√©rents DataFrame en un seul                
    df_merge_accident = caracs
        .merge(vehicules, on='Num_Acc', how='left', suffixes=('', '_veh')) 
        .merge(usagers, on='Num_Acc', how='left', suffixes=('', '_usag')) 
        .merge(lieux, on='Num_Acc', how='left', suffixes=('', '_lieu'))
    """, language="python")

    st.write("### Gestion des valeurs manquantes et des doublons")
    st.write("""Le premier d√©fi auquel nous avons √©t√© confront√© lors du pr√©-traitement a √©t√© la **gestion des valeurs manquantes et des doublons**.\n""")
    st.write("""Ces doublons provenaient du fait qu‚Äôun m√™me accident pouvait appara√Ætre plusieurs fois dans le jeu de donn√©es, notamment lorsqu‚Äôil √©tait d√©crit au niveau des **v√©hicules** ou des **usagers**.\n""")
    st.write("""Comme notre objectif √©tait d‚Äôanalyser la gravit√© de l‚Äôaccident, nous avons r√©encod√© et agr√©g√© certaines variables afin d‚Äôobtenir un jeu de donn√©es unique, contenant **une seule ligne par accident**.\n
 """)
    st.write("""Par ailleurs, les accidents comportent un grand nombre de facteurs ‚Äî tels que le lieu, la pr√©sence √©ventuelle de pi√©tons, l‚Äô√©quipement de s√©curit√©, ou encore le type de v√©hicule ‚Äî ce qui a entra√Æn√© la pr√©sence de **nombreuses valeurs manquantes dans certaines variables**.      
    """)
    
    st.write("### Gestion des Nans")
    st.write("""Le DataFrame final comportait plusieurs colonnes inutiles pour notre analyse que nous avons supprim√©s directement""")
    
    st.code("""
    # On supprime les colonnes inutiles               
    df_merge_accident = df_merge_accident.drop(['voie', 'annee','adr', 'gps', 'lat', 'long', 'vosp', 'v1', 'v2', 'pr', 'pr1', 'plan', 'env1'], axis = 1)
    """, language="python")

    st.code("""
    # On supprime les nans lorsqu'il y en a tr√®s peu             
    df_merge_accident = df_merge_accident.dropna(subset=['atm', 'col', 'com', 'catr', 'choc', 'manv', 'senc'])
    """, language="python")

    st.write("""Afin d‚Äô√©viter les biais li√©s aux valeurs manquantes, nous avons appliqu√© un **remplissage diff√©renci√© selon le type de variable** :
    les champs cat√©goriels inconnus ont √©t√© cod√©s par **-1**, les valeurs binaires absentes par **0**, et certaines variables num√©riques par leur **valeur m√©diane**.
            """)

    with st.expander("Afficher le code"):
        st.code("""
        ## On remplace les Nans de obs et obm par -1
        df_merge_accident[['obs', 'obsm']] = df_merge_accident[['obs', 'obsm']].fillna(-1)
            
        # lartpc : NaN = pas de TPC
        df_merge_accident['lartpc'] = df_merge_accident['lartpc'].fillna(0)

        # larrout : NaN = inconnu / non mesur√©
        df_merge_accident['larrout'] = df_merge_accident['larrout'].fillna(-1)

        # prof : inconnu
        df_merge_accident['prof'] = df_merge_accident['prof'].fillna(-1)

        # nbv : m√©diane 
        df_merge_accident['nbv'] = df_merge_accident['nbv'].fillna(df_merge_accident['nbv'].median())

        # circ : inconnu
        df_merge_accident['circ'] = df_merge_accident['circ'].fillna(-1)

        # infra : NaN = aucun am√©nagement particulier
        df_merge_accident['infra'] = df_merge_accident['infra'].fillna(0)

        # situ : inconnu
        df_merge_accident['situ'] = df_merge_accident['situ'].fillna(-1)""", language="python")

    st.write("""Afin de mieux pouvoir utiliser certaines variables, mais √©galement afin de supprimer les doublons, et de n'obtenir qu'une ligne par accident, nous avons r√©encod√© plusieurs variables""")

    st.write("### Transformation des variables 's√©curit√©', et 'num_veh'")
    st.write("""Nous avons **fusionn√© les informations de la variable s√©curit√©**, qui distinguait la pr√©sence et l‚Äôutilisation d‚Äôun √©quipement √† travers une valeur num√©rique, afin d‚Äôobtenir **une seule variable** indiquant simplement **si un √©quipement a √©t√© utilis√© ou non**.""")
    st.write("""De plus, la colonne **num_veh a √©t√© supprim√©e** et remplac√©e par une variable correspondant au **nombre de v√©hicules impliqu√©s dans chaque accident**.
    """)
    with st.expander("Exemple code variable s√©curit√©"):
        st.code("""
        #Si √©quipement utilis√©, on l'indique. Si pas utilis√© on met 0, si non d√©terminable, on met 3.
        def decode_secu(val):
            if pd.isna(val):
                return -1
            val = int(val)
            equipement = val // 10
            usage = val % 10
            if usage == 1:
                return equipement
            elif usage == 2:
                return 0
            elif usage == 3:
                return -1
            else:
                return -1  # cas impr√©vu
        df_merge_accident['secu'] = df_merge_accident['secu'].apply(decode_secu).astype('Int64')
            """, language="python")


    st.write("### R√©encodage")
    st.write("""Afin de supprimer les doublons, et de n'obtenir qu'une ligne par accident, nous avont r√©encoder plusieurs variables.""")

    st.write("""Nous avons cr√©√© plusieurs familles afin de regrouper les variables et avons cr√©√© un OneHotEncoder lorsque n√©cessaire""")

    st.write("""Il √©tait parfois necessaire d'effectuer des tests de Chi et Cramer pour comprendre l'importance des variables par rapport √† la variable cible.
    Nous avons √©galement d√ª analyser leur r√©partition pour comprendre comment les encoder""")

    with st.expander("Exemple de code pour la variable choc"):
        st.code("""
        ##Colonne choc, on r√©duit le nombre de colonnes 
        ## Cr√©er les 4 familles directement
        df_merge_accident['choc_avant']    = df_merge_accident['choc'].isin([1,2,3]).astype(int)
        df_merge_accident['choc_arriere']  = df_merge_accident['choc'].isin([4,5,6]).astype(int)
        df_merge_accident['choc_lateral']  = df_merge_accident['choc'].isin([7,8]).astype(int)
        df_merge_accident['choc_multiple'] = (df_merge_accident['choc'] == 9).astype(int)

        ## Agr√©ger pour avoir une ligne par accident
        choc_flags = (df_merge_accident.groupby('Num_Acc', as_index=False)
                        [['choc_avant','choc_arriere','choc_lateral','choc_multiple']]
                        .max())
        """, language="python")

    st.write("###### Variable surf")
    st.write("""
    - Normal : 1 
    - D√©favorable : 0   
    - Inconnu : -1""")

    st.write("###### Variable Sexe")
    st.write("""On a divis√© la colonne sexe en deux colonnes : Homme et Femme""")

    st.write("###### Variable place")
    st.write("""
    - Conducteur : place 1
    - Passager avant : place 2
    - Passager arri√®re : places 3 √† 6
    - Autres / ind√©termin√© : places 7, 8, 9, 0""")

    st.write("###### Variable Trajet")
    st.write("""
    - 0: "Inconnu"
    - 1: "Travail"        
    - 4: "Professionnel"   
    - 5: "Loisirs"          
    - 2, 3, 9: "Autre"
            """)          

    st.write("###### Variable √¢ge")
    st.write("""
    Nous avons v√©rifi√© si les valeurs aberrantes provenaient de fautes de frappe pouvant √™tre corrig√©es, ce qui n‚Äô√©tait pas le cas.

    Comme avec les Nans, elles repr√©sentaient **moins de 1%** des donn√©es, nous avons choisi de les √©liminer, en les rempla√ßant par la m√©diane ¬±5.
    Enfin, nous avons regroup√© les individus par classes d‚Äô√¢ge afin de faciliter l‚Äôanalyse.
    - age < 18: "Enfant"
    - age < 30: "Jeune"
    - age < 60: "Adulte"
    - age > 60: "Senior"
            """)

    st.write("### La variable cible : Variable gravit√© :")
    st.write("""Variable gravit√©
    La variable gravit√© est la variable que nous souhaitons pr√©dire, nous l'avons r√©parti en 4 classes :
    - 1:"Indemne" 
    - 2:"Tu√©"
    - 3:"Bless√© hospitalis√©"
    - 4:"Bless√© l√©ger" 
    Nous avons ensuite cr√©√©e une variable, en s√©lectionnant la gravit√© maximale de l'accident  """)

    st.code("""label_grav = {1:"Indemne", 2:"Tu√©", 3:"Bless√© hospitalis√©", 4:"Bless√© l√©ger"}
    ordre_gravite = {1:0, 4:1, 3:2, 2:3}  # √©chelle: indemne < l√©ger < hosp < tu√©

    u = df_merge_accident.copy()

    # Colonnes indicatrices par usager
    u['tue']   = (u['grav'] == 2).astype(int)
    u['hosp']  = (u['grav'] == 3).astype(int)
    u['leger'] = (u['grav'] == 4).astype(int)
    u['indem'] = (u['grav'] == 1).astype(int)

    # Gravit√© max
    u['grav_order'] = u['grav'].map(ordre_gravite)

    accident = u.groupby('Num_Acc').agg(grav_order_max = ('grav_order', 'max')).reset_index()""", language="python")

    st.write("### Traitement des variables cat√©gorielles et r√©duction de la colin√©arit√© :")
    st.write("""Lorsque des OneHotEncoding avait √©t√© appliqu√©, nous avons supprim√© une modalit√© de r√©f√©rence pour chaque variable afin d‚Äô√©viter la colin√©arit√©.
    Les colonnes redondantes ou apportant des informations similaires ont √©galement √©t√© retir√©es pour simplifier et stabiliser le jeu de donn√©es.
    """)

    st.write("###### Variable hrmn")                  
    st.write("""Nous avons encod√© hrmn en deux variables "heure" et "minute".      
    Nous avons supprim√© minute.""")

    st.write("### Train/test split")
    st.code("""##On split
    from sklearn.model_selection import train_test_split
    X = df_fin.drop(columns=["grav_order_max", "Num_Acc", "an", "minute"])
    y = df_fin["grav_order_max"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
    """, language="python")

    st.write("### Encodage des variables cat√©gorielles")
    st.write("""Les colonnes heure, jour et mois cr√©ant probablement une mauvais information, nous les avons trait√©s comme des variables cat√©gorielles
    """)
    st.code("""cat_cols = ["mois", "jour", "heure"]
    num_cols = X.columns.difference(cat_cols)

    # Pr√©traitement cat√©gorielles (One-Hot)
    cat_tf = Pipeline([("ohe", OneHotEncoder(handle_unknown="ignore", drop="first"))])

    # ColumnTransformer
    preprocess = ColumnTransformer([
        ("num", "passthrough", num_cols),
        ("cat", cat_tf, cat_cols),
    ])
            
    # Fit uniquement sur le train
    preprocess.fit(X_train)

    # Transforme train et test
    X_train_transformed = preprocess.transform(X_train)
    X_test_transformed = preprocess.transform(X_test)
            """, language="python")

    st.write("### Standardisation et r√©√©quilibrage des classes")
    st.write("""Nous avons remarqu√© que nous avions un d√©s√©quilibre des classes, que nous avons trait√© 
    """)

    st.write("""Comme nous avons rencontr√© √† plusieurs reprise des probl√®mes de m√©moire, nous avons d'abord opt√© pour un undersampling l√©ger, puis ensuite un SMOTE afin de r√©√©quilibrer 
    """)

    st.write("#### Standardisation ")
    st.code("""
    #Standardisation

    scaler = StandardScaler(with_mean=False)
    scaler.fit(X_train_transformed)

    X_train_scaled = scaler.transform(X_train_transformed)
    X_test_scaled = scaler.transform(X_test_transformed)
    """, language="python")

    st.write("#### Oversampling et Undersampling ")
    st.code("""
    from collections import Counter
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import SMOTE

    # Undersampling l√©ger
    # On r√©duit un peu la classe majoritaire sans tomber au niveau des minoritaires
    cnt = Counter(y_train)
    strategy_under = {
        2: int(cnt[2] * 0.5),  
        3: int(cnt[3] * 0.8),  
        4: cnt[4]              
    }

    rus = RandomUnderSampler(sampling_strategy=strategy_under, random_state=42)
    X_train_under, y_train_under = rus.fit_resample(X_train_scaled, y_train)
    print("Apr√®s undersampling :", Counter(y_train_under))

    # Oversampling (SMOTE) pour √©quilibrer
    smote = SMOTE(sampling_strategy='auto', random_state=42, k_neighbors=3)
    X_train_bal, y_train_bal = smote.fit_resample(X_train_under, y_train_under)
    """, language="python")



with tab3:
    st.markdown("#### Dataviz")
    #############################################################################
    ##                                DataViz                                  ##
    #############################################################################
            
    #####
    ## Nombre d'accident par ann√©es (=>sample => 1000 lignes = 1000 accidents)
    #####
            
    # Chargement de graphes issus du notebook
    #for name in ["Evol_acc_annee.png","Evol_acc_mois.png","Repart_moment.png", "Repart_age.png", "Repart_sexe.png", "Repart_Trajet.png", "Repart_Gravit√©.png","Dist_agg_grav.png", "Dist_int_grav.png", "Dist_lum_grav.png", "Dist_secu_grav.png", "Corr_var_expl_Grav.png", "Corr_var_expl.png"]
     
    col = st.columns([1,12,1])[1]
    with col:
         # Evol accidents/ann√©e
         name="Evol_acc_annee.png"
         st.image(f"reports/{name}", width=400, use_container_width=True)
         st.markdown("""
         **Analyse :** 
         - Baisse nette et continue entre 2005 et 2018 - co√Øncide avec l'installation de plus de 500 radars en 2005
         """)
         st.divider()

    # Evol accidents/mois
    with col:
         name="Evol_acc_mois.png"
         st.image(f"reports/{name}", width=400, use_container_width=True)
         st.markdown("""
         **Analyse :** 
         - Accidents plus fr√©quents entre avril et juillet : pic estival li√© √† l'augmentation des d√©placements (vacances, loisirs, + motards/cyclistes)  
         - Niveaux plus faibles en hiver (moins de trajets)
         """)
         st.divider()

    # Repart/moment
    with col:
         name="Repart_moment.png"
         st.image(f"reports/{name}", width=400, use_container_width=True)
         st.markdown("""
         **Analyse :** 
         - L'apr√®s-midi est la p√©riode la plus accidentog√®ne (38.5%) : corr√©l√© √† un trafic √©lev√© et √† un rythme de circulation actif  
         - Le soir suit en intensit√© (baisse de luminosit√© + fatigue accrue)
         - Le matin a une contribution mod√©r√©e  
         - La nuit (0h-6h) est la moins fr√©quent√©e, mais les accidents qui y surviennent peuvent √™tre plus graves  
         """)
         st.divider()

    # Repart/age
    with col:
         name="Repart_age.png"
         st.image(f"reports/{name}", width=400, use_container_width=True)
         st.markdown("""
         **Analyse :** 
         - Les **adultes** constituent la majorit√© des usagers accident√©s, en lien avec leur exposition plus forte au trafic quotidien
         - Les **jeunes** repr√©sentent une part importante : comportements √† risque et exp√©rience moindre 
         - Les **seniors** et **enfants** sont moins pr√©sents en nombre, mais plus vuln√©rables en cas d'accident
         """)
         st.divider()
     
    # Repart/sexe
    with col:
         name="Repart_sexe.png"
         st.image(f"reports/{name}", width=400, use_container_width=True)
         st.markdown("""
         **Analyse :** 
         - Les **hommes** repr√©sentent environ deux tiers des usagers impliqu√©s dans un accident
         - Cette surrepr√©sentation peut s'expliquer par une plus forte exposition et des comportements √† risque plus fr√©quent (vitesse, alcool, ..)
         - Les **femmes** apparaissent moins impliqu√©es, ce qui traduit √† la fois une exposition moindre et des comportements plus prudents
         """)
         st.divider()
     
    # Repart/Trajet
    with col:
         name="Repart_Trajet.png"
         st.image(f"reports/{name}", width=400, use_container_width=True)
         st.markdown("""
         **Analyse :** 
         - Les **trajets de loisirs** sont majoritaires parmi les usagers impliqu√©s, 
          refl√©tant une **plus forte exposition au risque** lors des d√©placements non professionnels
         - Les **trajets domicile-travail** restent fr√©quents mais sur des distances plus courtes
         - Les **trajets professionnels** sont minoritaires, bien que potentiellement plus graves
         - Une part importante de **trajets "inconnus"** correspond √† des donn√©es non renseign√©es
         """)
         st.divider()
     
    # Repart/Gravit√©
    with col:
         name="Repart_Gravit√©.png"
         st.image(f"reports/{name}", width=400, use_container_width=True)
         st.markdown("""
         **Analyse :** 
         - La majorit√© des accidents impliquent des **usagers indemnes**
         - Les **bless√©s l√©gers (incl. hospitalis√©s)** repr√©sentent environ un tiers des cas
         - Les **accidents mortels** sont minoritaires
         - Cette distribution est d√©s√©quilibr√©e : ce sera un **enjeu cl√© pour la mod√©lisation** (classes minoritaires)
         """)
         st.divider()

    # Distribution agg/gravite
    with col:
         name="Dist_agg_grav.png"
         st.image(f"reports/{name}", width=400, use_container_width=True)
         st.markdown("""
         **Analyse :** 
         - Les **accidents hors agglom√©ration** sont proportionnellement **plus graves** (davantage de bless√©s graves et de tu√©s)
         - En **agglom√©ration**, les accidents sont **plus fr√©quents mais souvent moins violents**
         - Ce contraste met en √©vidence le r√¥le de la **vitesse** et des **infrastructures** sur la gravit√©
         """)
         st.divider()
     
    # Distribution int/gravite
    with col:
         name="Dist_int_grav.png"
         st.image(f"reports/{name}", width=400, use_container_width=True)
         with st.expander("D√©finition des labels `int` (type d'intersection)"):
             st.markdown("""
             | Code | Type d'intersection |
             |------|----------------------|
             | 0 | Hors intersection |
             | 1 | Intersection en X |
             | 2 | Intersection en T |
             | 3 | Intersection en Y |
             | 4 | Carrefour √† sens giratoire |
             | 5 | Place |
             | 6 | Passage √† niveau |
             | 7 | Autre intersection |
             | 8 | √âchangeur ou bretelle d'autoroute |
             | 9 | Intersection multiple |
             """)
         st.markdown("""
         **Analyse :** 
         - Les intersections **simples (X, T, Y)** concentrent le plus grand nombre d'accidents
         - Les **giratoires** g√©n√®rent beaucoup d'accidents mais **peu graves** (gr√¢ce √† des vitesses mod√©r√©es)
         - Les **√©changeurs, passages √† niveau ou intersections multiples** sont moins fr√©quents,
           mais leurs accidents sont souvent **plus violents**
         """)
         st.divider()

    # Distribution lum/gravite
    with col:
         name="Dist_lum_grav.png"
         st.image(f"reports/{name}", width=400, use_container_width=True)
         with st.expander("D√©finition des valeurs `lum`"):
            st.markdown("""
             | Code | Type d'intersection |
             |------|----------------------|
             | 1 | Plein jour |
             | 2 | Aube / cr√©puscule |
             | 3 | Nuit sans √©clairage |
             | 4 | Nuit avec √©clairage |
             | 5 | Nuit sans √©clairage allum√© |
            """)
         st.markdown("""
         **Analyse :** 
         - La **gravit√© augmente** lorsque la luminosit√© diminue
         - Les accidents survenant **la nuit sans √©clairage** sont proportionnellement **les plus graves**
         - En **plein jour**, les accidents sont plus nombreux mais souvent **moins s√©v√®res**
         - La **visibilit√©** est donc un facteur d√©terminant dans la gravit√© des accidents
         """)
         st.divider()
     
    # Distribution secu/gravite
    with col:
         name="Dist_secu_grav.png"
         st.image(f"reports/{name}", width=400, use_container_width=True)
         with st.expander("D√©finition des valeurs `secu`"):
             st.markdown("""
              | Code | Signification |
              |------|----------------|
              | -1 | Donn√©e manquante / non renseign√©e |
              | 0 | Aucun dispositif de s√©curit√© |
              | 1 | Ceinture de s√©curit√© |
              | 2 | Casque |
              | 3 | Dispositif enfant |
              | 4 | Gilet r√©fl√©chissant |
              | 9 | Autre dispositif |
              """)
         st.markdown("""
         **Analyse :** 
         - L'absence de dispositif de s√©curit√© (`secu = 0`) est clairement associ√©e √† une **gravit√© plus forte**
         - Le **port de la ceinture ou du casque** r√©duit fortement la proportion de tu√©s
         - Ces r√©sultats confirment l'impact majeur des **√©quipements de s√©curit√©** dans la pr√©vention des d√©c√®s routiers
         """)
         st.divider()

    # Corr var expl/gravite
    with col:
         name="Corr_var_expl_Grav.png"
         st.image(f"reports/{name}", width=400, use_container_width=True)
         st.markdown("""
         **Analyse :** 
         - Aucune variable avec corr√©lation forte (> 0.3), mais certaines tendances directionnelles
         - La gravit√© d√©pend d'interactions complexes
         """)
         st.divider()

    # Corr var expl
    with col:
         name="Corr_var_expl.png"
         st.image(f"reports/{name}", width=400, use_container_width=True)
         st.markdown("""
         **Analyse :** 
         - Aucune corr√©lation forte (> 0.8) entre variables explicatives n'est observ√©e
         - Cela confirme l'absence de **multicolin√©arit√© majeure** dans le jeu de donn√©es
         - Chaque variable peut donc apporter une **information compl√©mentaire** utile √† la mod√©lisation
         """)
         st.divider()



